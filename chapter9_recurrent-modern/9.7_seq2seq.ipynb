{
 "cells": [
  {
   "cell_type": "code",
   "id": "bc9aa4b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:31.967521Z",
     "iopub.status.busy": "2023-08-18T07:15:31.966534Z",
     "iopub.status.idle": "2023-08-18T07:15:33.959337Z",
     "shell.execute_reply": "2023-08-18T07:15:33.958486Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-04-08T06:46:48.614665Z",
     "start_time": "2025-04-08T06:46:47.427873Z"
    }
   },
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:33.963601Z",
     "iopub.status.busy": "2023-08-18T07:15:33.962917Z",
     "iopub.status.idle": "2023-08-18T07:15:33.969272Z",
     "shell.execute_reply": "2023-08-18T07:15:33.968489Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-02-10T05:47:45.994629Z",
     "start_time": "2025-02-10T05:47:45.978653Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 29,
   "source": [
    "#用于序列到序列学习的循环神经网络编码器\n",
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        X = self.embedding(X)\n",
    "        X = X.permute(1, 0, 2)\n",
    "        output, state = self.rnn(X)\n",
    "        return output, state"
   ],
   "id": "3dbfb3ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T05:47:49.873383Z",
     "start_time": "2025-02-10T05:47:49.862359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# permute的用法\n",
    "import torch\n",
    "\n",
    "# 假设有一个形状为 (2, 3, 4) 的张量\n",
    "temp = torch.randn(2, 3, 4)\n",
    "print(\"原始张量的形状:\", temp.shape)\n",
    "\n",
    "# 使用 permute 重新排列维度\n",
    "temp_permuted = temp.permute(1, 0, 2)\n",
    "print(\"重新排列维度后的形状:\", temp_permuted.shape)\n",
    "\n",
    "# 打印原始张量和重新排列后的张量\n",
    "print(\"原始张量:\\n\", temp)\n",
    "print(\"重新排列后的张量:\\n\", temp_permuted)"
   ],
   "id": "cdf234596f6b713",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量的形状: torch.Size([2, 3, 4])\n",
      "重新排列维度后的形状: torch.Size([3, 2, 4])\n",
      "原始张量:\n",
      " tensor([[[ 0.5321, -1.1520,  0.7683, -1.5174],\n",
      "         [-0.3946,  0.5486,  0.3019,  0.8487],\n",
      "         [-0.4512, -0.7616, -0.1691,  0.2097]],\n",
      "\n",
      "        [[ 2.6866, -0.0147,  0.1441, -0.4080],\n",
      "         [ 1.6003,  1.0527,  0.0644, -0.1015],\n",
      "         [-0.8578,  0.1393,  0.5955,  0.3398]]])\n",
      "重新排列后的张量:\n",
      " tensor([[[ 0.5321, -1.1520,  0.7683, -1.5174],\n",
      "         [ 2.6866, -0.0147,  0.1441, -0.4080]],\n",
      "\n",
      "        [[-0.3946,  0.5486,  0.3019,  0.8487],\n",
      "         [ 1.6003,  1.0527,  0.0644, -0.1015]],\n",
      "\n",
      "        [[-0.4512, -0.7616, -0.1691,  0.2097],\n",
      "         [-0.8578,  0.1393,  0.5955,  0.3398]]])\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "1780ca82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:33.972667Z",
     "iopub.status.busy": "2023-08-18T07:15:33.972142Z",
     "iopub.status.idle": "2023-08-18T07:15:34.003637Z",
     "shell.execute_reply": "2023-08-18T07:15:34.002907Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-02-10T05:50:27.263172Z",
     "start_time": "2025-02-10T05:50:27.247159Z"
    }
   },
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "# eval()用于将模型设置为评估模式\n",
    "encoder.eval()\n",
    "# batch_size = 4, num_steps = 7\n",
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "# output的形状:(num_steps,batch_size,num_hiddens)\n",
    "output.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 16])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "32a2c1d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.007123Z",
     "iopub.status.busy": "2023-08-18T07:15:34.006595Z",
     "iopub.status.idle": "2023-08-18T07:15:34.011456Z",
     "shell.execute_reply": "2023-08-18T07:15:34.010716Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-02-10T05:50:30.813188Z",
     "start_time": "2025-02-10T05:50:30.799182Z"
    }
   },
   "source": [
    "# state的形状:(num_layers,batch_size,num_hiddens)\n",
    "state.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 16])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "09143bb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.014841Z",
     "iopub.status.busy": "2023-08-18T07:15:34.014327Z",
     "iopub.status.idle": "2023-08-18T07:15:34.021372Z",
     "shell.execute_reply": "2023-08-18T07:15:34.020591Z"
    },
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-02-10T04:16:56.201130Z",
     "start_time": "2025-02-10T04:16:56.187111Z"
    }
   },
   "source": [
    "# 解码器\n",
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    # 解码器的输入就是编码器的输出\n",
    "    def forward(self, X, state):\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        # 广播context，使其具有与X相同的num_steps\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # output的形状:(batch_size,num_steps,vocab_size)\n",
    "        # state的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "ad17a24d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.024844Z",
     "iopub.status.busy": "2023-08-18T07:15:34.024212Z",
     "iopub.status.idle": "2023-08-18T07:15:34.034277Z",
     "shell.execute_reply": "2023-08-18T07:15:34.033517Z"
    },
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-02-10T04:16:56.233114Z",
     "start_time": "2025-02-10T04:16:56.219108Z"
    }
   },
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "decoder.eval()\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "# output的形状为\n",
    "output.shape, state.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "57c5a5f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.037911Z",
     "iopub.status.busy": "2023-08-18T07:15:34.037256Z",
     "iopub.status.idle": "2023-08-18T07:15:34.044866Z",
     "shell.execute_reply": "2023-08-18T07:15:34.044120Z"
    },
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-02-10T04:16:56.265108Z",
     "start_time": "2025-02-10T04:16:56.251108Z"
    }
   },
   "source": [
    "#@save\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "sequence_mask(X, torch.tensor([1, 2]))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "f6b25824",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "(**我们还可以使用此函数屏蔽最后几个轴上的所有项。**)如果愿意，也可以使用指定的非零值来替换这些项。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fbb003c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.048373Z",
     "iopub.status.busy": "2023-08-18T07:15:34.047745Z",
     "iopub.status.idle": "2023-08-18T07:15:34.054283Z",
     "shell.execute_reply": "2023-08-18T07:15:34.053539Z"
    },
    "origin_pos": 36,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-02-10T04:16:56.345108Z",
     "start_time": "2025-02-10T04:16:56.332111Z"
    }
   },
   "source": [
    "X = torch.ones(2, 3, 4)\n",
    "sequence_mask(X, torch.tensor([1, 2]), value=-1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.],\n",
       "         [-1., -1., -1., -1.]],\n",
       "\n",
       "        [[ 1.,  1.,  1.,  1.],\n",
       "         [ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "ecf893af",
   "metadata": {
    "origin_pos": 39
   },
   "source": [
    "现在，我们可以[**通过扩展softmax交叉熵损失函数来遮蔽不相关的预测**]。\n",
    "最初，所有预测词元的掩码都设置为1。\n",
    "一旦给定了有效长度，与填充词元对应的掩码将被设置为0。\n",
    "最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0da33ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.057946Z",
     "iopub.status.busy": "2023-08-18T07:15:34.057267Z",
     "iopub.status.idle": "2023-08-18T07:15:34.062428Z",
     "shell.execute_reply": "2023-08-18T07:15:34.061664Z"
    },
    "origin_pos": 41,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-02-10T04:16:56.421791Z",
     "start_time": "2025-02-10T04:16:56.408792Z"
    }
   },
   "source": [
    "#@save\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带遮蔽的softmax交叉熵损失函数\"\"\"\n",
    "    # pred的形状：(batch_size,num_steps,vocab_size)\n",
    "    # label的形状：(batch_size,num_steps)\n",
    "    # valid_len的形状：(batch_size,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "f6b7600c",
   "metadata": {
    "origin_pos": 44
   },
   "source": [
    "我们可以创建三个相同的序列来进行[**代码健全性检查**]，\n",
    "然后分别指定这些序列的有效长度为$4$、$2$和$0$。\n",
    "结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "65239ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.065956Z",
     "iopub.status.busy": "2023-08-18T07:15:34.065339Z",
     "iopub.status.idle": "2023-08-18T07:15:34.073758Z",
     "shell.execute_reply": "2023-08-18T07:15:34.072755Z"
    },
    "origin_pos": 46,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-02-10T04:16:56.483791Z",
     "start_time": "2025-02-10T04:16:56.475791Z"
    }
   },
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long),\n",
    "     torch.tensor([4, 2, 0]))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.1513, 0.0000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "6bdc4e96",
   "metadata": {
    "origin_pos": 49
   },
   "source": [
    "## [**训练**]\n",
    ":label:`sec_seq2seq_training`\n",
    "\n",
    "在下面的循环训练过程中，如 :numref:`fig_seq2seq`所示，\n",
    "特定的序列开始词元（“&lt;bos&gt;”）和\n",
    "原始的输出序列（不包括序列结束词元“&lt;eos&gt;”）\n",
    "拼接在一起作为解码器的输入。\n",
    "这被称为*强制教学*（teacher forcing），\n",
    "因为原始的输出序列（词元的标签）被送入解码器。\n",
    "或者，将来自上一个时间步的*预测*得到的词元作为解码器的当前输入。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9d7b922e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.077404Z",
     "iopub.status.busy": "2023-08-18T07:15:34.076756Z",
     "iopub.status.idle": "2023-08-18T07:15:34.087405Z",
     "shell.execute_reply": "2023-08-18T07:15:34.086461Z"
    },
    "origin_pos": 51,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-02-10T04:16:56.529791Z",
     "start_time": "2025-02-10T04:16:56.509791Z"
    }
   },
   "source": [
    "#@save\n",
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"训练序列到序列模型\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "\n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                     xlim=[10, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # 训练损失总和，词元数量\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                          device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 强制教学\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()\t# 损失函数的标量进行“反向传播”\n",
    "            d2l.grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "        f'tokens/sec on {str(device)}')"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "fe583c85",
   "metadata": {
    "origin_pos": 54
   },
   "source": [
    "现在，在机器翻译数据集上，我们可以\n",
    "[**创建和训练一个循环神经网络“编码器－解码器”模型**]用于序列到序列的学习。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "79f585d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.091791Z",
     "iopub.status.busy": "2023-08-18T07:15:34.090975Z",
     "iopub.status.idle": "2023-08-18T07:16:11.767145Z",
     "shell.execute_reply": "2023-08-18T07:16:11.765998Z"
    },
    "origin_pos": 55,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-02-10T04:17:14.778827Z",
     "start_time": "2025-02-10T04:17:14.654826Z"
    }
   },
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, device = 0.005, 300, d2l.try_gpu()\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "net = d2l.EncoderDecoder(encoder, decoder)\n",
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ],
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xaf in position 33: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m batch_size, num_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m      3\u001B[0m lr, num_epochs, device \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.005\u001B[39m, \u001B[38;5;241m300\u001B[39m, d2l\u001B[38;5;241m.\u001B[39mtry_gpu()\n\u001B[1;32m----> 5\u001B[0m train_iter, src_vocab, tgt_vocab \u001B[38;5;241m=\u001B[39m \u001B[43md2l\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_data_nmt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m encoder \u001B[38;5;241m=\u001B[39m Seq2SeqEncoder(\u001B[38;5;28mlen\u001B[39m(src_vocab), embed_size, num_hiddens, num_layers,\n\u001B[0;32m      7\u001B[0m                         dropout)\n\u001B[0;32m      8\u001B[0m decoder \u001B[38;5;241m=\u001B[39m Seq2SeqDecoder(\u001B[38;5;28mlen\u001B[39m(tgt_vocab), embed_size, num_hiddens, num_layers,\n\u001B[0;32m      9\u001B[0m                         dropout)\n",
      "File \u001B[1;32mC:\\Program Files\\Python38\\lib\\site-packages\\d2l\\torch.py:927\u001B[0m, in \u001B[0;36mload_data_nmt\u001B[1;34m(batch_size, num_steps, num_examples)\u001B[0m\n\u001B[0;32m    923\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_data_nmt\u001B[39m(batch_size, num_steps, num_examples\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m600\u001B[39m):\n\u001B[0;32m    924\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the iterator and the vocabularies of the translation dataset.\u001B[39;00m\n\u001B[0;32m    925\u001B[0m \n\u001B[0;32m    926\u001B[0m \u001B[38;5;124;03m    Defined in :numref:`subsec_mt_data_loading`\"\"\"\u001B[39;00m\n\u001B[1;32m--> 927\u001B[0m     text \u001B[38;5;241m=\u001B[39m preprocess_nmt(\u001B[43mread_data_nmt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    928\u001B[0m     source, target \u001B[38;5;241m=\u001B[39m tokenize_nmt(text, num_examples)\n\u001B[0;32m    929\u001B[0m     src_vocab \u001B[38;5;241m=\u001B[39m d2l\u001B[38;5;241m.\u001B[39mVocab(source, min_freq\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m    930\u001B[0m                           reserved_tokens\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<pad>\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<bos>\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<eos>\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32mC:\\Program Files\\Python38\\lib\\site-packages\\d2l\\torch.py:859\u001B[0m, in \u001B[0;36mread_data_nmt\u001B[1;34m()\u001B[0m\n\u001B[0;32m    857\u001B[0m data_dir \u001B[38;5;241m=\u001B[39m d2l\u001B[38;5;241m.\u001B[39mdownload_extract(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfra-eng\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    858\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(data_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfra.txt\u001B[39m\u001B[38;5;124m'\u001B[39m), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m--> 859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'gbk' codec can't decode byte 0xaf in position 33: illegal multibyte sequence"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "b24c26df",
   "metadata": {
    "origin_pos": 56
   },
   "source": [
    "## [**预测**]\n",
    "\n",
    "为了采用一个接着一个词元的方式预测输出序列，\n",
    "每个解码器当前时间步的输入都将来自于前一时间步的预测词元。\n",
    "与训练类似，序列开始词元（“&lt;bos&gt;”）\n",
    "在初始时间步被输入到解码器中。\n",
    "该预测过程如 :numref:`fig_seq2seq_predict`所示，\n",
    "当输出序列的预测遇到序列结束词元（“&lt;eos&gt;”）时，预测就结束了。\n",
    "\n",
    "![使用循环神经网络编码器-解码器逐词元地预测输出序列。](../img/seq2seq-predict.svg)\n",
    ":label:`fig_seq2seq_predict`\n",
    "\n",
    "我们将在 :numref:`sec_beam-search`中介绍不同的序列生成策略。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7510bee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:16:11.771151Z",
     "iopub.status.busy": "2023-08-18T07:16:11.770496Z",
     "iopub.status.idle": "2023-08-18T07:16:11.779631Z",
     "shell.execute_reply": "2023-08-18T07:16:11.778678Z"
    },
    "origin_pos": 58,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n",
    "                    device, save_attention_weights=False):\n",
    "    \"\"\"序列到序列模型的预测\"\"\"\n",
    "    # 在预测时将net设置为评估模式\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n",
    "        src_vocab['<eos>']]\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # 添加批量轴\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # 添加批量轴\n",
    "    dec_X = torch.unsqueeze(torch.tensor(\n",
    "        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # 保存注意力权重（稍后讨论）\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        # 一旦序列结束词元被预测，输出序列的生成就完成了\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71773ad1",
   "metadata": {
    "origin_pos": 61
   },
   "source": [
    "## 预测序列的评估\n",
    "\n",
    "我们可以通过与真实的标签序列进行比较来评估预测序列。\n",
    "虽然 :cite:`Papineni.Roukos.Ward.ea.2002`\n",
    "提出的BLEU（bilingual evaluation understudy）\n",
    "最先是用于评估机器翻译的结果，\n",
    "但现在它已经被广泛用于测量许多应用的输出序列的质量。\n",
    "原则上说，对于预测序列中的任意$n$元语法（n-grams），\n",
    "BLEU的评估都是这个$n$元语法是否出现在标签序列中。\n",
    "\n",
    "我们将BLEU定义为：\n",
    "\n",
    "$$ \\exp\\left(\\min\\left(0, 1 - \\frac{\\mathrm{len}_{\\text{label}}}{\\mathrm{len}_{\\text{pred}}}\\right)\\right) \\prod_{n=1}^k p_n^{1/2^n},$$\n",
    ":eqlabel:`eq_bleu`\n",
    "\n",
    "其中$\\mathrm{len}_{\\text{label}}$表示标签序列中的词元数和\n",
    "$\\mathrm{len}_{\\text{pred}}$表示预测序列中的词元数，\n",
    "$k$是用于匹配的最长的$n$元语法。\n",
    "另外，用$p_n$表示$n$元语法的精确度，它是两个数量的比值：\n",
    "第一个是预测序列与标签序列中匹配的$n$元语法的数量，\n",
    "第二个是预测序列中$n$元语法的数量的比率。\n",
    "具体地说，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$\n",
    "和预测序列$A$、$B$、$B$、$C$、$D$，\n",
    "我们有$p_1 = 4/5$、$p_2 = 3/4$、$p_3 = 1/3$和$p_4 = 0$。\n",
    "\n",
    "根据 :eqref:`eq_bleu`中BLEU的定义，\n",
    "当预测序列与标签序列完全相同时，BLEU为$1$。\n",
    "此外，由于$n$元语法越长则匹配难度越大，\n",
    "所以BLEU为更长的$n$元语法的精确度分配更大的权重。\n",
    "具体来说，当$p_n$固定时，$p_n^{1/2^n}$\n",
    "会随着$n$的增长而增加（原始论文使用$p_n^{1/n}$）。\n",
    "而且，由于预测的序列越短获得的$p_n$值越高，\n",
    "所以 :eqref:`eq_bleu`中乘法项之前的系数用于惩罚较短的预测序列。\n",
    "例如，当$k=2$时，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$\n",
    "和预测序列$A$、$B$，尽管$p_1 = p_2 = 1$，\n",
    "惩罚因子$\\exp(1-6/2) \\approx 0.14$会降低BLEU。\n",
    "\n",
    "[**BLEU的代码实现**]如下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9135ade0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:16:11.784109Z",
     "iopub.status.busy": "2023-08-18T07:16:11.783827Z",
     "iopub.status.idle": "2023-08-18T07:16:11.791568Z",
     "shell.execute_reply": "2023-08-18T07:16:11.790396Z"
    },
    "origin_pos": 62,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def bleu(pred_seq, label_seq, k):  #@save\n",
    "    \"\"\"计算BLEU\"\"\"\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    for n in range(1, k + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[' '.join(pred_tokens[i: i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c57898",
   "metadata": {
    "origin_pos": 63
   },
   "source": [
    "最后，利用训练好的循环神经网络“编码器－解码器”模型，\n",
    "[**将几个英语句子翻译成法语**]，并计算BLEU的最终结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "653f0dd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:16:11.796025Z",
     "iopub.status.busy": "2023-08-18T07:16:11.795107Z",
     "iopub.status.idle": "2023-08-18T07:16:11.818936Z",
     "shell.execute_reply": "2023-08-18T07:16:11.817788Z"
    },
    "origin_pos": 64,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va !, bleu 1.000\n",
      "i lost . => j'ai perdu ., bleu 1.000\n",
      "he's calm . => il est riche ., bleu 0.658\n",
      "i'm home . => je suis en retard ?, bleu 0.447\n"
     ]
    }
   ],
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, attention_weight_seq = predict_seq2seq(\n",
    "        net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
    "    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": [],
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
